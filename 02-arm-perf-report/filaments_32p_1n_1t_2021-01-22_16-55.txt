Command:        /cosma/home/do008/dc-park1/STORM/storm3d/filaments
Resources:      1 node (32 physical, 64 logical cores per node)
Memory:         503 GiB per node
Tasks:          32 processes
Machine:        b109.pri.cosma7.alces.network
Start time:     Fri Jan 22 16:55:58 2021
Total time:     294 seconds (about 5 minutes)
Full path:      /cosma/home/do008/dc-park1/STORM/storm3d

Summary: filaments is Compute-bound in this configuration
Compute:                                     68.3% |======|
MPI:                                         30.6% |==|
I/O:                                          1.1% ||
This application run was Compute-bound. A breakdown of this time and advice for investigating further is in the CPU section below. 

CPU:
A breakdown of the 68.3% CPU time:
Scalar numeric ops:                          34.9% |==|
Vector numeric ops:                           0.7% ||
Memory accesses:                             64.4% |=====|
The per-core performance is memory-bound. Use a profiler to identify time-consuming loops and check their cache performance.
Little time is spent in vectorized instructions. Check the compiler's vectorization advice to see why key loops could not be vectorized.

MPI:
A breakdown of the 30.6% MPI time:
Time in collective calls:                     0.8% ||
Time in point-to-point calls:                99.2% |=========|
Effective process collective rate:             214 bytes/s
Effective process point-to-point rate:         218 MB/s
Most of the time is spent in point-to-point calls with an average transfer rate. Using larger messages and overlapping communication and computation may increase the effective transfer rate.

I/O:
A breakdown of the 1.1% I/O time:
Time in reads:                               24.5% |=|
Time in writes:                              75.5% |=======|
Effective process read rate:                  7.98 MB/s
Effective process write rate:                 2.82 MB/s
Most of the time is spent in write operations with a very low effective transfer rate. This may be caused by contention for the filesystem or inefficient access patterns. Use an I/O profiler to investigate which write calls are affected.

Threads:
A breakdown of how multiple threads were used:
Computation:                                  0.0% |
Synchronization:                              0.0% |
Physical core utilization:                   98.9% |=========|
System load:                                102.4% |=========|
No measurable time is spent in multithreaded code.

Memory:
Per-process memory usage may also affect scaling:
Mean process memory usage:                    73.1 MiB
Peak process memory usage:                    89.4 MiB
Peak node memory usage:                       3.0% ||
The peak node memory usage is very low. Running with fewer MPI processes and more data on each process may be more efficient.

Energy:
A breakdown of how energy was used:
CPU:                                      not supported
System:                                   not supported
Mean node power:                          not supported
Peak node power:                              0.00 W
Energy metrics are not available on this system.
CPU metrics are not supported (no intel_rapl module)

